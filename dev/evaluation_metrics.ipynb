{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet-1k Evaluation\n",
    "\n",
    "            # Top-1 Accuracy\n",
    "            # Top-5 Accuracy\n",
    "            # Precision\n",
    "            # Recall\n",
    "            # F1-Score\n",
    "            # Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the current directory to root directory\n",
    "new_directory = \"../\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify the current directory has changed\n",
    "updated_directory = os.getcwd()\n",
    "print(\"Updated Directory:\", updated_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### JSON format\n",
    "\n",
    "json_result = {\n",
    "    \"topk_accuracy\": {\n",
    "        \"top1_accuracy_raw\" : None,\n",
    "        \"top5_accuracy_raw\" : None,\n",
    "        \"n_samples\": None\n",
    "    },\n",
    "    \"precision_recall_f1_mAP\": {\n",
    "        \"0\": {\n",
    "            \"precision_raw\": None,\n",
    "            \"recall_raw\": None,\n",
    "            \"f1_raw\": None,\n",
    "            \"mAP_raw\": None,\n",
    "            \"n_samples\": None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topk_accuracy.top1_accuracy_raw': {0: 15},\n",
       " 'topk_accuracy.top5_accuracy_raw': {0: 35},\n",
       " 'topk_accuracy.n_samples': {0: 55},\n",
       " 'precision_recall_f1_mAP.0.precision_raw': {0: 75},\n",
       " 'precision_recall_f1_mAP.0.recall_raw': {0: 95},\n",
       " 'precision_recall_f1_mAP.0.f1_raw': {0: 115},\n",
       " 'precision_recall_f1_mAP.0.mAP_raw': {0: 135},\n",
       " 'precision_recall_f1_mAP.0.n_samples': {0: 155},\n",
       " 'precision_recall_f1_mAP.1.precision_raw': {0: 70},\n",
       " 'precision_recall_f1_mAP.1.recall_raw': {0: 90},\n",
       " 'precision_recall_f1_mAP.1.f1_raw': {0: 110},\n",
       " 'precision_recall_f1_mAP.1.mAP_raw': {0: 130},\n",
       " 'precision_recall_f1_mAP.1.n_samples': {0: 150}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### sum 2 jsons\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "json1 = {\n",
    "    \"topk_accuracy\": {\n",
    "        \"top1_accuracy_raw\": 10,\n",
    "        \"top5_accuracy_raw\": 20,\n",
    "        \"n_samples\": 30\n",
    "    },\n",
    "    \"precision_recall_f1_mAP\": {\n",
    "        \"0\": {\n",
    "            \"precision_raw\": 40,\n",
    "            \"recall_raw\": 50,\n",
    "            \"f1_raw\": 60,\n",
    "            \"mAP_raw\": 70,\n",
    "            \"n_samples\": 80\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"precision_raw\": 35,\n",
    "            \"recall_raw\": 45,\n",
    "            \"f1_raw\": 55,\n",
    "            \"mAP_raw\": 65,\n",
    "            \"n_samples\": 75\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "json2 = {\n",
    "    \"topk_accuracy\": {\n",
    "        \"top1_accuracy_raw\": 5,\n",
    "        \"top5_accuracy_raw\": 15,\n",
    "        \"n_samples\": 25\n",
    "    },\n",
    "    \"precision_recall_f1_mAP\": {\n",
    "        \"0\": {\n",
    "            \"precision_raw\": 35,\n",
    "            \"recall_raw\": 45,\n",
    "            \"f1_raw\": 55,\n",
    "            \"mAP_raw\": 65,\n",
    "            \"n_samples\": 75\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"precision_raw\": 35,\n",
    "            \"recall_raw\": 45,\n",
    "            \"f1_raw\": 55,\n",
    "            \"mAP_raw\": 65,\n",
    "            \"n_samples\": 75\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Flatten the JSON & Convert the flattened JSON to a Pandas DataFrame\n",
    "\n",
    "flattened_json = pd.json_normalize(json1)\n",
    "df1 = pd.DataFrame.from_dict(flattened_json)\n",
    "\n",
    "flattened_json = pd.json_normalize(json2)\n",
    "df2 = pd.DataFrame.from_dict(flattened_json)\n",
    "\n",
    "\n",
    "# Sum the corresponding parameters using pandas\n",
    "\n",
    "result = df1.add(df2, fill_value=0).to_dict()\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topk_accuracy.top1_accuracy_raw</th>\n",
       "      <th>topk_accuracy.top5_accuracy_raw</th>\n",
       "      <th>topk_accuracy.n_samples</th>\n",
       "      <th>precision_recall_f1_mAP.0.precision_raw</th>\n",
       "      <th>precision_recall_f1_mAP.0.recall_raw</th>\n",
       "      <th>precision_recall_f1_mAP.0.f1_raw</th>\n",
       "      <th>precision_recall_f1_mAP.0.mAP_raw</th>\n",
       "      <th>precision_recall_f1_mAP.0.n_samples</th>\n",
       "      <th>precision_recall_f1_mAP.1.precision_raw</th>\n",
       "      <th>precision_recall_f1_mAP.1.recall_raw</th>\n",
       "      <th>precision_recall_f1_mAP.1.f1_raw</th>\n",
       "      <th>precision_recall_f1_mAP.1.mAP_raw</th>\n",
       "      <th>precision_recall_f1_mAP.1.n_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topk_accuracy.top1_accuracy_raw  topk_accuracy.top5_accuracy_raw  \\\n",
       "0                               10                               20   \n",
       "1                                5                               15   \n",
       "\n",
       "   topk_accuracy.n_samples  precision_recall_f1_mAP.0.precision_raw  \\\n",
       "0                       30                                       40   \n",
       "1                       25                                       35   \n",
       "\n",
       "   precision_recall_f1_mAP.0.recall_raw  precision_recall_f1_mAP.0.f1_raw  \\\n",
       "0                                    50                                60   \n",
       "1                                    45                                55   \n",
       "\n",
       "   precision_recall_f1_mAP.0.mAP_raw  precision_recall_f1_mAP.0.n_samples  \\\n",
       "0                                 70                                   80   \n",
       "1                                 65                                   75   \n",
       "\n",
       "   precision_recall_f1_mAP.1.precision_raw  \\\n",
       "0                                       35   \n",
       "1                                       35   \n",
       "\n",
       "   precision_recall_f1_mAP.1.recall_raw  precision_recall_f1_mAP.1.f1_raw  \\\n",
       "0                                    45                                55   \n",
       "1                                    45                                55   \n",
       "\n",
       "   precision_recall_f1_mAP.1.mAP_raw  precision_recall_f1_mAP.1.n_samples  \n",
       "0                                 65                                   75  \n",
       "1                                 65                                   75  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### list of multiple jsons\n",
    "\n",
    "flattened_json = pd.json_normalize([json1, json2])\n",
    "df_all = pd.DataFrame.from_dict(flattened_json)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topk_accuracy.top1_accuracy_raw             15\n",
       "topk_accuracy.top5_accuracy_raw             35\n",
       "topk_accuracy.n_samples                     55\n",
       "precision_recall_f1_mAP.0.precision_raw     75\n",
       "precision_recall_f1_mAP.0.recall_raw        95\n",
       "precision_recall_f1_mAP.0.f1_raw           115\n",
       "precision_recall_f1_mAP.0.mAP_raw          135\n",
       "precision_recall_f1_mAP.0.n_samples        155\n",
       "precision_recall_f1_mAP.1.precision_raw     70\n",
       "precision_recall_f1_mAP.1.recall_raw        90\n",
       "precision_recall_f1_mAP.1.f1_raw           110\n",
       "precision_recall_f1_mAP.1.mAP_raw          130\n",
       "precision_recall_f1_mAP.1.n_samples        150\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### sum alltogether\n",
    "\n",
    "column_sums = df_all.sum()\n",
    "print(type(column_sums))\n",
    "column_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_sums[\"topk_accuracy.n_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7419354838709677"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_class_0 = column_sums[\"precision_recall_f1_mAP.0.f1_raw\"] / column_sums[\"precision_recall_f1_mAP.0.n_samples\"]\n",
    "f1_score_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import datasets\n",
    "import typing\n",
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics_imagenet1k(predictions, references):\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    references = np.array(references)\n",
    "    labels = list(range(1000))\n",
    "    counts_each_label = [np.count_nonzero(references == label) for label in labels]\n",
    "\n",
    "\n",
    "    # Top-1 accuracy\n",
    "    k = 1\n",
    "    topk_indices = np.argsort(-predictions, axis=1)[:, :k] # descending order\n",
    "    top1_accuracy_raw = top_k_accuracy_score(references, topk_indices, k=k)*len(predictions)\n",
    "\n",
    "    # Top-5 Accuracy\n",
    "    k = 5\n",
    "    topk_indices = np.argsort(-predictions, axis=1)[:, :k] # descending order\n",
    "    top5_accuracy_raw = top_k_accuracy_score(references, topk_indices, k=k)*len(predictions)\n",
    "\n",
    "    # Precision\n",
    "\n",
    "    precision_result = precision_score(\n",
    "                    y_pred = predictions.argmax(axis=1), \n",
    "                    y_true = references, \n",
    "                    labels=labels, \n",
    "                    pos_label=1, \n",
    "                    average=None, \n",
    "                    zero_division=1)\n",
    "    \n",
    "    # Recall\n",
    "\n",
    "    recall_result = recall_score(\n",
    "                    y_pred = predictions.argmax(axis=1), \n",
    "                    y_true = references, \n",
    "                    labels=labels, \n",
    "                    pos_label=1, \n",
    "                    average=None, \n",
    "                    zero_division=1)\n",
    "    \n",
    "\n",
    "    # F1-Score\n",
    "\n",
    "    f1_result = f1_score(\n",
    "                    y_pred = predictions.argmax(axis=1), \n",
    "                    y_true = references, \n",
    "                    labels=labels, \n",
    "                    pos_label=1, \n",
    "                    average=None, \n",
    "                    zero_division=1)\n",
    "    \n",
    "\n",
    "    # Mean Average Precision (mAP)\n",
    "\n",
    "    mAP_result = average_precision_score(\n",
    "                    y_pred = predictions.argmax(axis=1), \n",
    "                    y_score = references, \n",
    "                    labels=labels, \n",
    "                    pos_label=1, \n",
    "                    average=None, \n",
    "                    zero_division=1)\n",
    "\n",
    "\n",
    "    # json initialize\n",
    "\n",
    "    results = {\n",
    "        \"topk_accuracy\": {\n",
    "            \"top1_accuracy_raw\" : top1_accuracy_raw,\n",
    "            \"top5_accuracy_raw\" : top5_accuracy_raw,\n",
    "            \"n_samples\": len(predictions)\n",
    "        },\n",
    "        \"precision_recall_f1_mAP\": {}\n",
    "        }\n",
    "    \n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "        n_samples_of_label_i = counts_each_label[i]\n",
    "\n",
    "        labelwise_results = {\n",
    "            \"precision_raw\": precision_result[i]*n_samples_of_label_i,\n",
    "            \"recall_raw\": recall_result[i]*n_samples_of_label_i,\n",
    "            \"f1_raw\": f1_result[i]*n_samples_of_label_i,\n",
    "            \"mAP_raw\": mAP_result[i]*n_samples_of_label_i,\n",
    "            \"n_samples\": n_samples_of_label_i\n",
    "        }\n",
    "        results[\"precision_recall_f1_mAP\"][str(i)] = labelwise_results\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of JSON objects: {'sum': 210}\n"
     ]
    }
   ],
   "source": [
    "####### JSON sum\n",
    "\n",
    "import json\n",
    "\n",
    "# Example JSON objects\n",
    "json1 = {\"value1\": 10, \"value2\": 20}\n",
    "json2 = {\"value1\": 30, \"value2\": 40}\n",
    "json3 = {\"value1\": 50, \"value2\": 60}\n",
    "\n",
    "# Extract the values from each JSON object\n",
    "values = [json1[\"value1\"], json1[\"value2\"], json2[\"value1\"], json2[\"value2\"], json3[\"value1\"], json3[\"value2\"]]\n",
    "\n",
    "# Calculate the sum of the values\n",
    "sum_values = sum(values)\n",
    "\n",
    "# Create a new JSON object with the summed values\n",
    "sum_json = {\"sum\": sum_values}\n",
    "\n",
    "# Print the result\n",
    "print(\"Sum of JSON objects:\", sum_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "####### Top-1 Accuracy\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [3],\n",
    "    [4],\n",
    "    [1],\n",
    "])\n",
    "\n",
    "references = np.array([3, 4, 0])\n",
    "\n",
    "results = accuracyk.compute(predictions=predictions, references=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "2.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score, precision_score\n",
    "\n",
    "references = np.array([3, 4, 1])\n",
    "\n",
    "predictions = np.array([\n",
    "    [0.         , 0.30434783, 0.04347826, 0.13043478, 0.2173913 ],\n",
    "    [0.         , 0.09090909, 0.40909091, 0.36363636, 0.18181818],\n",
    "    [0.61538462, 0.30769231, 0.07692308, 0.07692308, 0.15384615]\n",
    "])\n",
    "\n",
    "top3_indices = np.argsort(-predictions, axis=1)[:, :3] # descending order\n",
    "\n",
    "top2 = top_k_accuracy_score(references, top3_indices, k=2) #, labels = [0,1,2,3,4])\n",
    "print(top2)\n",
    "\n",
    "top2_raw = top2* len(predictions)\n",
    "n_samples = len(predictions)\n",
    "print(top2_raw)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "####### Top-5 Accuracy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [0, 7, 1, 3, 5],\n",
    "    [0, 2, 9, 8, 4],\n",
    "    [8, 4, 1, 2, 3],\n",
    "])\n",
    "references = np.array([3, 4, 5])\n",
    "\n",
    "results = accuracyk.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 3]\n",
      " [2 3 4]\n",
      " [0 1 4]]\n"
     ]
    }
   ],
   "source": [
    "#### get top 3 (say top 5) from full row of 5 elements (say 1000)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predictions = np.array([\n",
    "    [0.         , 0.30434783, 0.04347826, 0.13043478, 0.2173913 ],\n",
    "    [0.         , 0.09090909, 0.40909091, 0.36363636, 0.18181818],\n",
    "    [0.61538462, 0.30769231, 0.07692308, 0.07692308, 0.15384615]\n",
    "])\n",
    "\n",
    "# Get the indices of the top 3 maximum values in each row\n",
    "top3_indices = np.argsort(-predictions, axis=1)[:, :3] # descending order\n",
    "\n",
    "\n",
    "print(top3_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([1., 0., 0., 1., 1.])}\n",
      "[1, 0, 0, 1, 1]\n",
      "label 0:\tprecision_accumulate = 1.0\tcount_accumulate = 1\n",
      "label 1:\tprecision_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 2:\tprecision_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 3:\tprecision_accumulate = 1.0\tcount_accumulate = 1\n",
      "label 4:\tprecision_accumulate = 1.0\tcount_accumulate = 1\n",
      "{'precision': 0.0}\n",
      "{'precision': 0.0}\n",
      "{'precision': 1.0}\n"
     ]
    }
   ],
   "source": [
    "####### Precision\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"precision\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [0, 7, 1, 3, 5],\n",
    "    [0, 2, 9, 8, 4],\n",
    "    [8, 4, 1, 1, 3],\n",
    "])\n",
    "references = np.array([3, 4, 0])\n",
    "\n",
    "results = metric.compute(predictions=predictions.argmax(axis=1), references=references,\n",
    "                         pos_label=1, average=None, zero_division=1, labels = [0,1,2,3,4])\n",
    "print(results)\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "counts = [np.count_nonzero(references == label) for label in labels]\n",
    "\n",
    "print(counts)\n",
    "\n",
    "for i in labels:\n",
    "    print(f\"label {i}:\\tprecision_accumulate = {results['precision'][i]* counts[i]}\\tcount_accumulate = {counts[i]}\")\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    results = metric.compute(predictions=predictions[i:i+1, :].argmax(axis=1), references=references[i:i+1],\n",
    "                            pos_label=1, average=\"weighted\", zero_division=0, labels = [0,1,2,3,4])\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': array([1., 1., 0., 1., 1.])}\n",
      "[2, 0, 0, 1, 1]\n",
      "label 0:\tprecision_accumulate = 2.0\tcount_accumulate = 2\n",
      "label 1:\tprecision_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 2:\tprecision_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 3:\tprecision_accumulate = 1.0\tcount_accumulate = 1\n",
      "label 4:\tprecision_accumulate = 1.0\tcount_accumulate = 1\n"
     ]
    }
   ],
   "source": [
    "### COUNT\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"precision\")\n",
    "\n",
    "predictions = np.array([3, 2, 0, 0])\n",
    "references = np.array([3, 4, 0, 0])\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "results = metric.compute(predictions=predictions, references=references,\n",
    "                         pos_label=1, average=None, zero_division=1, labels = [0,1,2,3,4])\n",
    "print(results)\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "counts = [np.count_nonzero(references == label) for label in labels]\n",
    "\n",
    "print(counts)\n",
    "\n",
    "for i in labels:\n",
    "    print(f\"label {i}:\\tprecision_accumulate = {results['precision'][i]* counts[i]}\\tcount_accumulate = {counts[i]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': array([1., 1., 1., 0., 0.])}\n",
      "[1, 0, 0, 1, 1]\n",
      "label 0:\trecall_accumulate = 1.0\tcount_accumulate = 1\n",
      "label 1:\trecall_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 2:\trecall_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 3:\trecall_accumulate = 0.0\tcount_accumulate = 1\n",
      "label 4:\trecall_accumulate = 0.0\tcount_accumulate = 1\n",
      "{'recall': 0.0}\n",
      "{'recall': 0.0}\n",
      "{'recall': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### Recall\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"recall\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [0, 7, 1, 3, 5],\n",
    "    [0, 2, 9, 8, 4],\n",
    "    [8, 4, 1, 1, 3],\n",
    "])\n",
    "references = np.array([3, 4, 0])\n",
    "\n",
    "results = metric.compute(predictions=predictions.argmax(axis=1), references=references,\n",
    "                         pos_label=1, average=None, zero_division=1, labels = [0,1,2,3,4])\n",
    "print(results)\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "counts = [np.count_nonzero(references == label) for label in labels]\n",
    "\n",
    "print(counts)\n",
    "\n",
    "for i in labels:\n",
    "    print(f\"label {i}:\\trecall_accumulate = {results['recall'][i]* counts[i]}\\tcount_accumulate = {counts[i]}\")\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    results = metric.compute(predictions=predictions[i:i+1, :].argmax(axis=1), references=references[i:i+1],\n",
    "                            pos_label=1, average=\"weighted\", zero_division=0, labels = [0,1,2,3,4])\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': array([1., 0., 0., 0., 0.])}\n",
      "[1, 0, 0, 1, 1]\n",
      "label 0:\tf1_accumulate = 1.0\tcount_accumulate = 1\n",
      "label 1:\tf1_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 2:\tf1_accumulate = 0.0\tcount_accumulate = 0\n",
      "label 3:\tf1_accumulate = 0.0\tcount_accumulate = 1\n",
      "label 4:\tf1_accumulate = 0.0\tcount_accumulate = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': array([0., 0., 0., 0., 0.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': array([0., 0., 0., 0., 0.])}\n",
      "{'f1': array([1., 0., 0., 0., 0.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### F1-Score\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [0, 7, 1, 3, 5],\n",
    "    [0, 2, 9, 8, 4],\n",
    "    [8, 4, 1, 1, 3],\n",
    "])\n",
    "references = np.array([3, 4, 0])\n",
    "\n",
    "results = metric.compute(predictions=predictions.argmax(axis=1), references=references,\n",
    "                         pos_label=1, average=None, labels = [0,1,2,3,4])\n",
    "print(results)\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "counts = [np.count_nonzero(references == label) for label in labels]\n",
    "\n",
    "print(counts)\n",
    "\n",
    "for i in labels:\n",
    "    print(f\"label {i}:\\tf1_accumulate = {results['f1'][i]* counts[i]}\\tcount_accumulate = {counts[i]}\")\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    results = metric.compute(predictions=predictions[i:i+1, :].argmax(axis=1), references=references[i:i+1],\n",
    "                            pos_label=1, average=None, labels = [0,1,2,3,4])\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 1.0\n",
      "class 1: 1.0\n",
      "class 2: -0.0\n",
      "class 3: -0.0\n",
      "class 4: 1.0\n",
      "class 5: -0.0\n",
      "class 6: -0.0\n",
      "class 7: -0.0\n",
      "class 8: 1.0\n",
      "class 9: -0.0\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# chanelcolgate/average_precision\n",
    "\n",
    "def calculate_map(predictions, label_ids):\n",
    "\n",
    "    # Get true labels\n",
    "    labels = label_ids\n",
    "\n",
    "    # Calculate average precision for each class\n",
    "    average_precisions = []\n",
    "    num_classes = predictions.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        print(\"class {}:\".format(class_idx), average_precision_score(labels == class_idx, predictions[:, class_idx],\n",
    "                                                                     average=None, pos_label=1))\n",
    "\n",
    "        average_precisions.append(average_precision_score(labels == class_idx, predictions[:, class_idx]))\n",
    "        # print(average_precisions)\n",
    "\n",
    "    # Calculate mean average precision (mAP)\n",
    "    map_score = sum(average_precisions) / num_classes\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "predictions = np.array([\n",
    "    [9, 4, 7, 8, 2, 0, 6, 5, 3, 1],\n",
    "    [9, 1, 7, 8, 0, 3, 2, 5, 6, 4],\n",
    "    [7, 0, 4, 8, 1, 6, 5, 2, 9, 3],\n",
    "    [0, 5, 7, 4, 9, 3, 2, 1, 8, 6],\n",
    "    [5, 9, 8, 2, 0, 7, 6, 3, 1, 4],\n",
    "]) # say, predictions in float\n",
    "\n",
    "# after argmax axis=1, result: [0 0 8 4 1]\n",
    "\n",
    "# references = np.array([3, 4, 0, 2, 1])\n",
    "\n",
    "references = np.array([0, 0, 8, 4, 1]) #(here, predictions == references)\n",
    "\n",
    "results = calculate_map(predictions=predictions, label_ids=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### writing compute metrics function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### sample evaluation script with focalnet tiny model online available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
