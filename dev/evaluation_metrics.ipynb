{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lvwerra/test',\n",
       " 'precision',\n",
       " 'code_eval',\n",
       " 'roc_auc',\n",
       " 'cuad',\n",
       " 'xnli',\n",
       " 'rouge',\n",
       " 'pearsonr',\n",
       " 'mse',\n",
       " 'super_glue',\n",
       " 'comet',\n",
       " 'cer',\n",
       " 'sacrebleu',\n",
       " 'mahalanobis',\n",
       " 'wer',\n",
       " 'competition_math',\n",
       " 'f1',\n",
       " 'recall',\n",
       " 'coval',\n",
       " 'mauve',\n",
       " 'xtreme_s',\n",
       " 'bleurt',\n",
       " 'ter',\n",
       " 'accuracy',\n",
       " 'exact_match',\n",
       " 'indic_glue',\n",
       " 'spearmanr',\n",
       " 'mae',\n",
       " 'squad',\n",
       " 'chrf',\n",
       " 'glue',\n",
       " 'perplexity',\n",
       " 'mean_iou',\n",
       " 'squad_v2',\n",
       " 'meteor',\n",
       " 'bleu',\n",
       " 'wiki_split',\n",
       " 'sari',\n",
       " 'frugalscore',\n",
       " 'google_bleu',\n",
       " 'bertscore',\n",
       " 'matthews_correlation',\n",
       " 'seqeval',\n",
       " 'trec_eval',\n",
       " 'rl_reliability',\n",
       " 'jordyvl/ece',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'cpllab/syntaxgym',\n",
       " 'lvwerra/bary_score',\n",
       " 'kaggle/amex',\n",
       " 'kaggle/ai4code',\n",
       " 'hack/test_metric',\n",
       " 'yzha/ctc_eval',\n",
       " 'codeparrot/apps_metric',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'daiyizheng/valid',\n",
       " 'poseval',\n",
       " 'erntkn/dice_coefficient',\n",
       " 'mgfrantz/roc_auc_macro',\n",
       " 'Vlasta/pr_auc',\n",
       " 'gorkaartola/metric_for_tp_fp_samples',\n",
       " 'idsedykh/metric',\n",
       " 'idsedykh/codebleu2',\n",
       " 'idsedykh/codebleu',\n",
       " 'idsedykh/megaglue',\n",
       " 'cakiki/ndcg',\n",
       " 'brier_score',\n",
       " 'Vertaix/vendiscore',\n",
       " 'GMFTBY/dailydialogevaluate',\n",
       " 'GMFTBY/dailydialog_evaluate',\n",
       " 'jzm-mailchimp/joshs_second_test_metric',\n",
       " 'ola13/precision_at_k',\n",
       " 'yulong-me/yl_metric',\n",
       " 'abidlabs/mean_iou',\n",
       " 'abidlabs/mean_iou2',\n",
       " 'KevinSpaghetti/accuracyk',\n",
       " 'NimaBoscarino/weat',\n",
       " 'ronaldahmed/nwentfaithfulness',\n",
       " 'Viona/infolm',\n",
       " 'kyokote/my_metric2',\n",
       " 'kashif/mape',\n",
       " 'Ochiroo/rouge_mn',\n",
       " 'giulio98/code_eval_outputs',\n",
       " 'leslyarun/fbeta_score',\n",
       " 'giulio98/codebleu',\n",
       " 'anz2/iliauniiccocrevaluation',\n",
       " 'zbeloki/m2',\n",
       " 'xu1998hz/sescore',\n",
       " 'mase',\n",
       " 'mape',\n",
       " 'smape',\n",
       " 'dvitel/codebleu',\n",
       " 'NCSOFT/harim_plus',\n",
       " 'JP-SystemsX/nDCG',\n",
       " 'sportlosos/sescore',\n",
       " 'Drunper/metrica_tesi',\n",
       " 'jpxkqx/peak_signal_to_noise_ratio',\n",
       " 'jpxkqx/signal_to_reconstrution_error',\n",
       " 'hpi-dhc/FairEval',\n",
       " 'nist_mt',\n",
       " 'lvwerra/accuracy_score',\n",
       " 'character',\n",
       " 'charcut_mt',\n",
       " 'ybelkada/cocoevaluate',\n",
       " 'harshhpareek/bertscore',\n",
       " 'posicube/mean_reciprocal_rank',\n",
       " 'bstrai/classification_report',\n",
       " 'omidf/squad_precision_recall',\n",
       " 'Josh98/nl2bash_m',\n",
       " 'BucketHeadP65/confusion_matrix',\n",
       " 'BucketHeadP65/roc_curve',\n",
       " 'yonting/average_precision_score',\n",
       " 'transZ/test_parascore',\n",
       " 'transZ/sbert_cosine',\n",
       " 'hynky/sklearn_proxy',\n",
       " 'xu1998hz/sescore_english_mt',\n",
       " 'xu1998hz/sescore_german_mt',\n",
       " 'xu1998hz/sescore_english_coco',\n",
       " 'xu1998hz/sescore_english_webnlg',\n",
       " 'unnati/kendall_tau_distance',\n",
       " 'r_squared',\n",
       " 'Viona/fuzzy_reordering',\n",
       " 'Viona/kendall_tau',\n",
       " 'lhy/hamming_loss',\n",
       " 'lhy/ranking_loss',\n",
       " 'Muennighoff/code_eval',\n",
       " 'yuyijiong/quad_match_score',\n",
       " 'Splend1dchan/cosine_similarity',\n",
       " 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       " 'Yeshwant123/mcc',\n",
       " 'transformersegmentation/segmentation_scores',\n",
       " 'sma2023/wil',\n",
       " 'chanelcolgate/average_precision',\n",
       " 'ckb/unigram',\n",
       " 'Felipehonorato/eer',\n",
       " 'manueldeprada/beer',\n",
       " 'tialaeMceryu/unigram',\n",
       " 'shunzh/apps_metric',\n",
       " 'He-Xingwei/sari_metric',\n",
       " 'langdonholmes/cohen_weighted_kappa',\n",
       " 'fschlatt/ner_eval',\n",
       " 'hyperml/balanced_accuracy',\n",
       " 'brian920128/doc_retrieve_metrics']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import list_evaluation_modules\n",
    "\n",
    "\n",
    "list_evaluation = list_evaluation_modules(module_type=\"metric\")\n",
    "list_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = ['lvwerra/test',\n",
    " 'precision',\n",
    " 'code_eval',\n",
    " 'roc_auc',\n",
    " 'cuad',\n",
    " 'xnli',\n",
    " 'rouge',\n",
    " 'pearsonr',\n",
    " 'mse',\n",
    " 'super_glue',\n",
    " 'comet',\n",
    " 'cer',\n",
    " 'sacrebleu',\n",
    " 'mahalanobis',\n",
    " 'wer',\n",
    " 'competition_math',\n",
    " 'f1',\n",
    " 'recall',\n",
    " 'coval',\n",
    " 'mauve',\n",
    " 'xtreme_s',\n",
    " 'bleurt',\n",
    " 'ter',\n",
    " 'accuracy',\n",
    " 'exact_match',\n",
    " 'indic_glue',\n",
    " 'spearmanr',\n",
    " 'mae',\n",
    " 'squad',\n",
    " 'chrf',\n",
    " 'glue',\n",
    " 'perplexity',\n",
    " 'mean_iou',\n",
    " 'squad_v2',\n",
    " 'meteor',\n",
    " 'bleu',\n",
    " 'wiki_split',\n",
    " 'sari',\n",
    " 'frugalscore',\n",
    " 'google_bleu',\n",
    " 'bertscore',\n",
    " 'matthews_correlation',\n",
    " 'seqeval',\n",
    " 'trec_eval',\n",
    " 'rl_reliability',\n",
    " 'jordyvl/ece',\n",
    " 'angelina-wang/directional_bias_amplification',\n",
    " 'cpllab/syntaxgym',\n",
    " 'lvwerra/bary_score',\n",
    " 'kaggle/amex',\n",
    " 'kaggle/ai4code',\n",
    " 'hack/test_metric',\n",
    " 'yzha/ctc_eval',\n",
    " 'codeparrot/apps_metric',\n",
    " 'mfumanelli/geometric_mean',\n",
    " 'daiyizheng/valid',\n",
    " 'poseval',\n",
    " 'erntkn/dice_coefficient',\n",
    " 'mgfrantz/roc_auc_macro',\n",
    " 'Vlasta/pr_auc',\n",
    " 'gorkaartola/metric_for_tp_fp_samples',\n",
    " 'idsedykh/metric',\n",
    " 'idsedykh/codebleu2',\n",
    " 'idsedykh/codebleu',\n",
    " 'idsedykh/megaglue',\n",
    " 'cakiki/ndcg',\n",
    " 'brier_score',\n",
    " 'Vertaix/vendiscore',\n",
    " 'GMFTBY/dailydialogevaluate',\n",
    " 'GMFTBY/dailydialog_evaluate',\n",
    " 'jzm-mailchimp/joshs_second_test_metric',\n",
    " 'ola13/precision_at_k',\n",
    " 'yulong-me/yl_metric',\n",
    " 'abidlabs/mean_iou',\n",
    " 'abidlabs/mean_iou2',\n",
    " 'KevinSpaghetti/accuracyk',\n",
    " 'NimaBoscarino/weat',\n",
    " 'ronaldahmed/nwentfaithfulness',\n",
    " 'Viona/infolm',\n",
    " 'kyokote/my_metric2',\n",
    " 'kashif/mape',\n",
    " 'Ochiroo/rouge_mn',\n",
    " 'giulio98/code_eval_outputs',\n",
    " 'leslyarun/fbeta_score',\n",
    " 'giulio98/codebleu',\n",
    " 'anz2/iliauniiccocrevaluation',\n",
    " 'zbeloki/m2',\n",
    " 'xu1998hz/sescore',\n",
    " 'mase',\n",
    " 'mape',\n",
    " 'smape',\n",
    " 'dvitel/codebleu',\n",
    " 'NCSOFT/harim_plus',\n",
    " 'JP-SystemsX/nDCG',\n",
    " 'sportlosos/sescore',\n",
    " 'Drunper/metrica_tesi',\n",
    " 'jpxkqx/peak_signal_to_noise_ratio',\n",
    " 'jpxkqx/signal_to_reconstrution_error',\n",
    " 'hpi-dhc/FairEval',\n",
    " 'nist_mt',\n",
    " 'lvwerra/accuracy_score',\n",
    " 'character',\n",
    " 'charcut_mt',\n",
    " 'ybelkada/cocoevaluate',\n",
    " 'harshhpareek/bertscore',\n",
    " 'posicube/mean_reciprocal_rank',\n",
    " 'bstrai/classification_report',\n",
    " 'omidf/squad_precision_recall',\n",
    " 'Josh98/nl2bash_m',\n",
    " 'BucketHeadP65/confusion_matrix',\n",
    " 'BucketHeadP65/roc_curve',\n",
    " 'yonting/average_precision_score',\n",
    " 'transZ/test_parascore',\n",
    " 'transZ/sbert_cosine',\n",
    " 'hynky/sklearn_proxy',\n",
    " 'xu1998hz/sescore_english_mt',\n",
    " 'xu1998hz/sescore_german_mt',\n",
    " 'xu1998hz/sescore_english_coco',\n",
    " 'xu1998hz/sescore_english_webnlg',\n",
    " 'unnati/kendall_tau_distance',\n",
    " 'r_squared',\n",
    " 'Viona/fuzzy_reordering',\n",
    " 'Viona/kendall_tau',\n",
    " 'lhy/hamming_loss',\n",
    " 'lhy/ranking_loss',\n",
    " 'Muennighoff/code_eval',\n",
    " 'yuyijiong/quad_match_score',\n",
    " 'Splend1dchan/cosine_similarity',\n",
    " 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
    " 'Yeshwant123/mcc',\n",
    " 'transformersegmentation/segmentation_scores',\n",
    " 'sma2023/wil',\n",
    " 'chanelcolgate/average_precision',\n",
    " 'ckb/unigram',\n",
    " 'Felipehonorato/eer',\n",
    " 'manueldeprada/beer',\n",
    " 'tialaeMceryu/unigram',\n",
    " 'shunzh/apps_metric',\n",
    " 'He-Xingwei/sari_metric',\n",
    " 'langdonholmes/cohen_weighted_kappa',\n",
    " 'fschlatt/ner_eval',\n",
    " 'hyperml/balanced_accuracy',\n",
    " 'brian920128/doc_retrieve_metrics']\n",
    "\n",
    "selected = [\"KevinSpaghetti/accuracyk\", \"precision\", \"recall\", \"f1\", ]\n",
    "\n",
    "\n",
    "            # Top-1 Accuracy\n",
    "            # Top-5 Accuracy\n",
    "            # Precision\n",
    "            # Recall\n",
    "            # F1-Score\n",
    "            # Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"trec_eval\", module_type: \"metric\", features: {'predictions': {'query': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'q0': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'docid': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'rank': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'score': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'system': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'references': {'query': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'q0': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'docid': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'rel': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}, usage: \"\"\"\n",
       "Calculates TREC evaluation scores based on a run and qrel.\n",
       "Args:\n",
       "    predictions: list containing a single run.\n",
       "    references: list containing a single qrel.\n",
       "Returns:\n",
       "    dict: TREC evaluation scores.\n",
       "Examples:\n",
       "    >>> trec = evaluate.load(\"trec_eval\")\n",
       "    >>> qrel = {\n",
       "    ...     \"query\": [0],\n",
       "    ...     \"q0\": [\"0\"],\n",
       "    ...     \"docid\": [\"doc_1\"],\n",
       "    ...     \"rel\": [2]\n",
       "    ... }\n",
       "    >>> run = {\n",
       "    ...     \"query\": [0, 0],\n",
       "    ...     \"q0\": [\"q0\", \"q0\"],\n",
       "    ...     \"docid\": [\"doc_2\", \"doc_1\"],\n",
       "    ...     \"rank\": [0, 1],\n",
       "    ...     \"score\": [1.5, 1.2],\n",
       "    ...     \"system\": [\"test\", \"test\"]\n",
       "    ... }\n",
       "    >>> results = trec.compute(references=[qrel], predictions=[run])\n",
       "    >>> print(results[\"P@5\"])\n",
       "    0.2\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metrics_ = load('trec_eval')\n",
    "metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracyk = evaluate.load(\"KevinSpaghetti/accuracyk\")\n",
    "\n",
    "predictions = np.array([\n",
    "    [0, 7, 1, 3, 5],\n",
    "    [0, 2, 9, 8, 4],\n",
    "    [8, 4, 1, 1, 3],\n",
    "])\n",
    "references = np.array([3, 4, 0])\n",
    "\n",
    "results = accuracyk.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: 1.0\n",
      "class 1: 1.0\n",
      "class 2: -0.0\n",
      "class 3: -0.0\n",
      "class 4: 1.0\n",
      "class 5: -0.0\n",
      "class 6: -0.0\n",
      "class 7: -0.0\n",
      "class 8: 1.0\n",
      "class 9: -0.0\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/media/toma/2TB_30May2023/OBJECT_DETECTION_L/object_detection/.venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def calculate_map(predictions, label_ids):\n",
    "\n",
    "    # Get true labels\n",
    "    labels = label_ids\n",
    "\n",
    "    # Calculate average precision for each class\n",
    "    average_precisions = []\n",
    "    num_classes = predictions.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        print(\"class {}:\".format(class_idx), average_precision_score(labels == class_idx, predictions[:, class_idx]))\n",
    "\n",
    "        average_precisions.append(average_precision_score(labels == class_idx, predictions[:, class_idx]))\n",
    "\n",
    "    # Calculate mean average precision (mAP)\n",
    "    map_score = sum(average_precisions) / num_classes\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "predictions = np.array([\n",
    "    [9, 4, 7, 8, 2, 0, 6, 5, 3, 1],\n",
    "    [9, 1, 7, 8, 0, 3, 2, 5, 6, 4],\n",
    "    [7, 0, 4, 8, 1, 6, 5, 2, 9, 3],\n",
    "    [0, 5, 7, 4, 9, 3, 2, 1, 8, 6],\n",
    "    [5, 9, 8, 2, 0, 7, 6, 3, 1, 4],\n",
    "]) # say, predictions in float\n",
    "\n",
    "# after argmax axis=1, result: [0 0 8 4 1]\n",
    "\n",
    "# references = np.array([3, 4, 0, 2, 1])\n",
    "\n",
    "references = np.array([0, 0, 8, 4, 1]) #(here, predictions == references)\n",
    "\n",
    "results = calculate_map(predictions=predictions, label_ids=references)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
